{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Random Forest.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1x43noBnDLHErcZ0po_pS7ERJv8H1blKa",
      "authorship_tag": "ABX9TyO8ijWYXVz0o48s9byiyIzA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnuttharadeSilva/t_analyzer_model/blob/master/Random_Forest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZO-wIjRBHiS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "220a77b6-1e32-44de-8862-c8025b0b3e95"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from string import punctuation \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJ1DpgsJACIZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('./drive/My Drive/tweet/tweets.csv', delimiter = ',',encoding='ISO-8859-1')\n",
        "#df2 = pd.read_csv('./drive/My Drive/tweet/reviews.csv', delimiter = ',',encoding='ISO-8859-1')\n",
        "#frames = [df1,df2]\n",
        "#df = pd.concat(frames)\n",
        "#df = pd.DataFrame(df.values, columns = ['Sentiment','SentimentText'])\n",
        "df = df.dropna()\n",
        "df = df.reset_index(drop=True)\n",
        "df_length =len(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99d5JkBVJAMr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "8c43ce08-7aa6-431f-b617-5735e231f94c"
      },
      "source": [
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>SentimentText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>I didn't realize it was THAT deep. Geez giv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>I hate it when any athlete appears to tear ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>i miss you guys too     i think i'm wearing...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>-- Meet your Meat http://bit.ly/15SSCI</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>My horsie is moving on Saturday morning.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>No Sat off...Need to work 6 days a week</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>Really Dont Like Doing my Room Its So Borin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>SOX!     Floyd was great, but relievers nee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>times by like a million</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>uploading pictures on friendster</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Sentiment                                      SentimentText\n",
              "0          0     I didn't realize it was THAT deep. Geez giv...\n",
              "1          0     I hate it when any athlete appears to tear ...\n",
              "2          0     i miss you guys too     i think i'm wearing...\n",
              "3          0             -- Meet your Meat http://bit.ly/15SSCI\n",
              "4          0           My horsie is moving on Saturday morning.\n",
              "5          0           No Sat off...Need to work 6 days a week \n",
              "6          0     Really Dont Like Doing my Room Its So Borin...\n",
              "7          0     SOX!     Floyd was great, but relievers nee...\n",
              "8          0                            times by like a million\n",
              "9          1                  uploading pictures on friendster "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_AVYJsmAFz-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = []\n",
        "stopwords = set(stopwords.words('english')+list(punctuation)+['AT_USER','URL'])\n",
        "for i in range(0, df_length):    \n",
        "    tweet = re.sub('[^a-zA-Z]', ' ', df['SentimentText'][i])\n",
        "    tweet = tweet.lower()\n",
        "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', tweet)\n",
        "    tweet = re.sub('@[^\\s]+', 'AT_USER', tweet)\n",
        "    #tweet = tweet.split()\n",
        "    tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
        "    tweet = tokenizer.tokenize(tweet)\n",
        "    #ps = PorterStemmer()\n",
        "    lmz = nltk.stem.WordNetLemmatizer()\n",
        "    tweet = [lmz.lemmatize(word) for word in tweet if not word in stopwords ]\n",
        "    tweet = ' '.join(tweet)\n",
        "    corpus.append(tweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6fQEEVQL5z9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "592172b2-e9e6-4f39-9b0d-8a12ff5bff36"
      },
      "source": [
        "corpus[0:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['realize deep geez give girl warning atleast',\n",
              " 'hate athlete appears tear acl live television',\n",
              " 'miss guy think wearing skinny jean cute sweater heel really sure today',\n",
              " 'meet meat http bit ly ssci',\n",
              " 'horsie moving saturday morning',\n",
              " 'sat need work day week',\n",
              " 'really dont like room boring sick wardrobe cant waiit till walk one yay',\n",
              " 'sox floyd great reliever need scolding',\n",
              " 'time like million',\n",
              " 'uploading picture friendster']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3GLWSBtAHbj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "cv = CountVectorizer()\n",
        "td = TfidfTransformer(use_idf=True, norm='l2',smooth_idf=True)\n",
        "docs = cv.fit_transform(corpus).toarray()\n",
        "X = td.fit_transform(docs).toarray()\n",
        "y = df.iloc[:, 0].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPsZ9nhTAIx5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugmXZ274cUcp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "f42127f8-b457-4abc-b22c-e1019cf2ebf8"
      },
      "source": [
        "#Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy')\n",
        "classifier.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='entropy', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYnNRg84caBP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_ycdAQN17KQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "42632b48-54bb-4ae7-e329-0d1466104249"
      },
      "source": [
        "#accuracy score\n",
        "from sklearn import metrics\n",
        "print('Accuracy: ',metrics.accuracy_score(y_test, y_pred))\n",
        "\n",
        "#confusion matrix\n",
        "confusion = metrics.confusion_matrix(y_test, y_pred)\n",
        "print(confusion)\n",
        "TP = confusion[1, 1]\n",
        "TN = confusion[0, 0]\n",
        "FP = confusion[0, 1]\n",
        "FN = confusion[1, 0]\n",
        "\n",
        "#error\n",
        "classification_error = (FP + FN) / float(TP + TN + FP + FN)\n",
        "print('Error: ',classification_error)\n",
        "\n",
        "#recall/sensitivity\n",
        "print('Sensitivity: ',metrics.recall_score(y_test, y_pred))\n",
        "\n",
        "#specificity\n",
        "specificity = TN / (TN + FP)\n",
        "print('Specificity: ',specificity)\n",
        "\n",
        "#false positive rate\n",
        "print('false positive rate: ',1 - specificity)\n",
        "\n",
        "#precision\n",
        "print('precision: ',metrics.precision_score(y_test, y_pred))\n",
        "\n",
        "#f1 score\n",
        "from sklearn.metrics import f1_score\n",
        "print('f1 score: ',f1_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.7132867132867133\n",
            "[[1706  504]\n",
            " [ 685 1252]]\n",
            "Error:  0.2867132867132867\n",
            "Sensitivity:  0.6463603510583377\n",
            "Specificity:  0.7719457013574661\n",
            "false positive rate:  0.2280542986425339\n",
            "precision:  0.7129840546697038\n",
            "f1 score:  0.678039534253994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUoS_-VKn7LI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.externals import joblib\n",
        "# joblib.dump(classifier, 'rf_new.pkl')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}